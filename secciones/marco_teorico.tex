\section{Marco Teórico}
\label{cap-marcot}

\subsection{Visión Computacional}
La visión computacional es una rama de la inteligencia artificial que busca emular la capacidad humana de interpretar información visual del entorno mediante algoritmos capaces de procesar imágenes y extraer conocimiento significativo. Este campo ha evolucionado de manera considerable en las últimas décadas, impulsado por avances en el aprendizaje profundo y el procesamiento de grandes volúmenes de datos visuales.\\

Desde un enfoque teórico, la visión computacional se fundamenta en principios de óptica, geometría proyectiva, aprendizaje automático y procesamiento de señales, disciplinas que en conjunto permiten modelar la relación entre el espacio tridimensional (3D) y su representación bidimensional (2D) en una imagen. Uno de los desafíos más relevantes es precisamente la recuperación de información tridimensional a partir de imágenes planas, lo cual constituye la base de tareas como la estimación de profundidad, reconstrucción 3D, localización y navegación autónoma.\\

Entre los principales paradigmas se encuentran la visión computacional estereoscópica, que emplea múltiples cámaras para inferir profundidad mediante triangulación directa entre puntos correspondientes en distintas vistas, y la visión computacional monocular, que enfrenta el reto de estimar la estructura 3D del entorno a partir de una única imagen o secuencia de imágenes de una sola cámara.\\ Este último enfoque, aunque más desafiante debido a la ambigüedad inherente en la proyección 2D, resulta especialmente atractivo por su bajo costo computacional y de hardware, y es el foco principal del presente trabajo. 

% Ambas subsecciones deberían ser una sola (Visión computacional + monocular deberían describir lo mismo)

\subsubsection{Visión Monocular}

La visión monocular se fundamenta en principios geométricos que permiten inferir información tridimensional a partir de proyecciones bidimensionales. En este contexto, la \textit{geometría proyectiva} proporciona el marco matemático que describe la formación de imágenes mediante una cámara, estableciendo cómo los puntos del espacio tridimensional se transforman en coordenadas del plano imagen. A diferencia de la geometría euclidiana, la geometría proyectiva no preserva distancias ni ángulos, sino relaciones de incidencia y proporciones invariantes, lo que la hace adecuada para modelar los efectos de la proyección perspectiva \cite{birchfield1998projective}.

El modelo más utilizado para representar este proceso es el modelo de \textit{cámara estenopeica} (o \textit{pinhole camera model}), el cual asume una lente ideal que proyecta los rayos de luz a través de un punto denominado \textit{centro óptico} sobre un plano imagen situado a una distancia focal $f$. Bajo este modelo, la proyección de un punto del espacio $P = (X, Y, Z)$ en coordenadas del mundo hacia un punto $p = (x, y)$ en el plano imagen se describe mediante la semejanza de triángulos \cite{szeliski2022computer}:

\begin{equation}
    \frac{x}{X} = \frac{f}{Z}, \quad \frac{y}{Y} = \frac{f}{Z}.
    \label{eq:pinhole}
\end{equation}

De esta relación se obtiene la transformación perspectiva directa:

\begin{equation}
    x = f \frac{X}{Z}, \quad y = f \frac{Y}{Z},
    \label{eq:projection_basic}
\end{equation}

la cual refleja que la proyección de un punto en el plano imagen depende de la razón entre su posición en el espacio y su profundidad $Z$. En términos geométricos, esto significa que, a medida que un objeto se aleja de la cámara (aumenta $Z$), su tamaño proyectado en la imagen disminuye de forma inversamente proporcional.

A partir de esta formulación se deriva una relación fundamental entre la distancia al objeto y su tamaño aparente. Considerando un objeto de altura real $H$ cuya proyección en la imagen tiene una altura $h$, se cumple la siguiente proporción \cite{shi2021geometry}:

\begin{equation}
    \frac{h}{f} = \frac{H}{Z} \quad \Rightarrow \quad Z = f \frac{H}{h}.
    \label{eq:distance_relation}
\end{equation}

Esta expresión constituye la base de la estimación de distancia mediante visión monocular, ya que permite inferir la profundidad $Z$ a partir del tamaño proyectado $h$, siempre que se conozca la distancia focal $f$ de la cámara y la altura real $H$ del objeto. En sistemas digitales, esta relación puede ajustarse considerando las dimensiones del sensor y el tamaño de píxel.


% Esta sección se descartaría, en su lugar hablar acerca de los modelos propuestos de Lanz dada su metodología - la utilizada para su modelo de estimación de distancias dadas las características de los objetos (Tamaño real - parámetros de la cámara)

%\subsection{Estimación de Profundidad}
%La estimación de profundidad monocular es una técnica de visión artificial que trata de reconstruir la representación de una escena en tres dimensiones partiendo de una única imagen en dos dimensiones. Este problema es por su naturaleza engañoso, ya que, para una misma proyección bidimensional, existen muchas configuraciones espacio temporales posibles. Más allá de esta ambigüedad subyacente, una serie de desarrollos teórico-experimentales han demostrado que es posible aprender a aproximarse a la verdad cuando se dispone de mucho contexto visual, modelos geométricos y técnicas de aprendizaje automático.\\
% Desde el punto de vista geométrico, el modelo de cámara pinhole actúa como la base para interpretar la proyección de un punto 3D sobre un plano de imagen 2D, como se detalló previamente. En este modelo, la falta de información estéreo o de múltiples vistas implica que la escala absoluta de la escena no puede determinarse directamente, lo que introduce el concepto de ambigüedad de escala. 

% \subsubsection{Ambigüedad de Escala}
% Al utilizar una única imagen, no se dispone de información suficiente para determinar la escala absoluta de la escena, lo que introduce la conocida ambigüedad de escala. Esta limitación implica que los sistemas de visión monocular solo pueden recuperar relaciones de profundidad relativas, a menos que se incorporen señales adicionales, como la altura conocida de la cámara, pistas contextuales o supuestos geométricos sobre el entorno \cite{hartley2004multiple}. En un sistema de visión monocular, la proyección de una escena tridimensional (3D) sobre una imagen bidimensional (2D) a través de una cámara pinhole implica una pérdida inevitable de información métrica. Específicamente, la proyección preserva la dirección de los rayos que conectan la cámara con los puntos del entorno, pero no su distancia exacta desde el centro óptico [18]. Por tanto, múltiples configuraciones espaciales distintas pueden producir la misma imagen proyectada, siempre que mantengan proporciones relativas equivalentes entre los puntos proyectados.\\
% Matemáticamente, si se tiene un punto 3D $\textbf{X}=(X,Y,Z,1)^T$ y su proyección en el plano de la imagen es $\textbf{x}=(x,y,1)^T$, se establece la relación proyectiva en (\ref{eq:projective_relation}).
% \begin{equation}
%    \textbf{x} \sim K[R|t]\textbf{X}
%    \label{eq:projective_relation}
% \end{equation}

% donde $K$ representa la matriz de parámetros intrínsecos de la cámara (distancia focal, centro óptico, etc.), y $[R|t]$ codifica la rotación y traslación del sistema de referencia de la cámara. La notación $\sim$ denota igualdad hasta escala, lo cual implica que los puntos proyectados sólo están definidos hasta un factor escalar desconocido.\\
% Dado que este sistema no incluye una referencia externa que permita fijar dicha escala, cualquier reconstrucción monocular de la escena resultará ambigua respecto al tamaño absoluto de los objetos o distancias reales. 

% Aquí se hablaría de las redes neuronales recurrentes y las variantes GRU y LSTM utilizadas.

\subsubsection{Modelo de Detección: YOLOv8}

El modelo de detección utilizado en este proyecto es YOLOv8, una de las versiones más recientes de la familia \textit{You Only Look Once}, desarrollada por Ultralytics. Este enfoque pertenece a los detectores de una sola etapa, donde la predicción de cajas delimitadoras y categorías se realiza en un único proceso inferencial sobre la imagen completa, permitiendo lograr velocidades en tiempo real sin sacrificar precisión \cite{bochkovskiy2020yolov4}.

A nivel conceptual, el detector toma una imagen de entrada 
$I \in \mathbb{R}^{H \times W \times 3}$ 
y produce un conjunto de predicciones 
$\{\hat{\mathbf{b}}_i, \hat{c}_i, \hat{p}_i\}_{i=1}^N$, 
donde $\hat{\mathbf{b}}_i = (\hat{x}_i,\hat{y}_i,\hat{w}_i,\hat{h}_i)$ corresponde a la caja delimitadora, $\hat{c}_i$ a la categoría predicha, y $\hat{p}_i$ a la probabilidad asociada.

YOLOv8 conserva la estructura clásica de los detectores modernos: un \textit{backbone} para extraer características jerárquicas, un \textit{neck} para fusionar información multiescala mediante arquitecturas tipo FPN/PAN, y una \textit{cabeza} de predicción encargada de estimar cajas, clases y objetividad \cite{ultralytics2023yolov8}.

Cada celda en los mapas de características predice un vector de salida del tipo:
\begin{equation}
    {p}_c = (o_c, \Delta x_c, \Delta y_c, \Delta w_c, \Delta h_c, {s}_c),
\end{equation}

donde $o_c$ es la probabilidad de presencia de objeto, 
$(\Delta x_c, \Delta y_c, \Delta w_c, \Delta h_c)$ 
son desplazamientos relativos para reconstruir la caja final, y 
$\mathbf{s}_c$ contiene las puntuaciones de clase.

La decodificación de una caja predicha se realiza mediante transformaciones paramétricas que ajustan los desplazamientos a la geometría del mapa de características:
\[
\hat{x}_c = \sigma(\Delta x_c) + c_x,\qquad
\hat{y}_c = \sigma(\Delta y_c) + c_y,
\]
\[
\hat{w}_c = e^{\Delta w_c} \cdot a_w,\qquad
\hat{h}_c = e^{\Delta h_c} \cdot a_h,
\]
donde $(c_x,c_y)$ representa el centro de la celda y $(a_w,a_h)$ son factores de escala adaptados al enfoque \textit{anchor-free} adoptado por YOLOv8 \cite{ultralytics2023yolov8}.

Tras la predicción, se aplica una etapa de post–procesado basada en \textit{Non-Maximum Suppression} (NMS) o \textit{Soft-NMS}, encargada de eliminar cajas redundantes y garantizar que el conjunto final de detecciones contenga instancias únicas y confiables.

\subsubsection{Modelo de seguimiento: DeepSORT}
El modelo de seguimiento DeepSORT integra un modelo dinámico clásico (filtro de Kalman con modelo de velocidad constante) con una métrica de asociación aprendida para mejorar la robustez frente a oclusiones y cambios de apariencia \cite{wojke2017simple}. DeepSORT opera sobre el conjunto de detecciones por cuadro provistas por el detector y resuelve, en línea, la correspondencia entre detecciones y trayectorias activas (tracks) mediante una combinación de información de movimiento y de apariencia.

A nivel formal, cada track mantiene una estimación del estado dinámico del objeto modelado por el vector de estado
\begin{equation}
    \mathbf{x} = \begin{bmatrix} x & y & a & h & v_x & v_y & v_a & v_h \end{bmatrix}^\top
\end{equation}
    
donde \(x,y\) son las coordenadas del centro de la caja, \(a\) la razón de aspecto (ancho/alto), \(h\) la altura de la caja, y \(v_x,v_y,v_a,v_h\) las correspondientes velocidades. El filtro de Kalman emplea un modelo de velocidad constante para predecir la dinámica del estado y cuantifica la incertidumbre de dicha predicción; de la predicción se obtiene la medida predicha \(\mathbf{y}_i\) para el track \(i\) y su covarianza (incertidumbre) \(S_i\).

Para asociar predicciones de tracks con detecciones observadas, DeepSORT construye una matriz de costos que combina dos componentes complementarios:

\begin{enumerate}
  \item \textbf{Componente de movimiento (Mahalanobis):} mide la discrepancia entre la detección \(\mathbf{d}_j\) y la predicción del filtro de Kalman \(\mathbf{y}_i\), teniendo en cuenta la incertidumbre \(S_i\). Su forma es
  \begin{equation}
    d^{2}(i,j) = (\mathbf{d}_j - \mathbf{y}_{i})^\top S_i^{-1}(\mathbf{d}_j - \mathbf{y}_{i})  
  \end{equation}
    
  donde \(\mathbf{d}_j\) es el vector- medición de la detección (tipicamente \([x,y,a,h]\)), \(\mathbf{y}_i\) la medición predicha para el track \(i\), y \(S_i\) la covarianza de innovación asociada a esa predicción. Intuitivamente, \(d^{2}(i,j)\) es una distancia normalizada por la incertidumbre: diferencias grandes respecto a la predicción o con baja confianza (covarianza pequeña) aumentan el costo.
  \item \textbf{Componente de apariencia:} evalúa la similitud entre la descripción de apariencia extraída de la detección y las descripciones almacenadas para el track. Se suele usar la distancia coseno entre descriptores L2-normalizados; de modo compacto:
  \begin{equation}
  c_{\text{app}}(i,j) \;=\; 1 - \frac{\mathbf{f}_i^\top \mathbf{f}_j}{\|\mathbf{f}_i\|\;\|\mathbf{f}_j\|}
  \end{equation}
    
  donde \(\mathbf{f}_j\) es el descriptor de la detección \(j\) y \(\mathbf{f}_i\) es el descriptor (o un promedio/galería de descriptores) asociado al track \(i\). Este término toma valores cercanos a 0 para apariencias muy similares y crece hacia 1 cuanto más diferentes son las apariencias.
\end{enumerate}

La matriz de costos final para la asociación suele combinar ambos términos mediante una suma ponderada:
\begin{equation}
c(i,j) \;=\; \lambda\, d^{2}(i,j) \;+\; (1-\lambda)\, c_{\text{app}}(i,j)
\end{equation}

con \(\lambda\in[0,1]\) un parámetro que controla la importancia relativa de la dinámica frente a la apariencia. Antes de resolver la asignación, DeepSORT aplica una \emph{puerta} (gating) basada en la distancia Mahalanobis: si \(d^{2}(i,j)\) excede un umbral estadístico (derivado de la distribución para el tamaño de la medición) la asociación se considera inviable y se descarta. Finalmente, sobre las asociaciones viables se resuelve el problema de asignación óptima, y las coincidencias resultantes actualizan los filtros de Kalman de cada track (paso de corrección) mientras que las no emparejadas crean nuevas trayectorias o actualizan el estado de tracks sin observación.


\subsection{Redes Neuronales Recurrentes}

Las redes neuronales recurrentes (RNN) pertenecen a una variante de arquitecturas diseñadas para procesar datos secuenciales donde la información útil en un instante depende de eventos previos. A diferencia de las redes feed-forward, las RNN incorporan un \emph{estado oculto} que actúa como memoria a corto plazo y se actualiza paso a paso a medida que la secuencia es procesada. Esta propiedad las vuelve adecuadas para tareas donde la dinámica temporal o la dependencia contextual son fundamentales.
El modelo de RNN se puede describir mediante las ecuaciones siguientes:
\begin{align}
    h_t &= \phi\big(W_{xh} x_t + W_{hh} h_{t-1} + b_h\big), \label{eq:rnn_state}\\
    y_t &= \psi\big(W_{hy} h_t + b_y\big), \label{eq:rnn_output}
\end{align}
donde \(x_t\in\mathbb{R}^{d_x}\) es la entrada, \(h_t\in\mathbb{R}^{d_h}\) el estado oculto, \(y_t\) la salida (si aplica), \(W_{xh}, W_{hh}, W_{hy}\) las matrices de pesos, \(b_h,b_y\) los sesgos, \(\phi\) una no linealidad (p.\,ej. \(\tanh\)) y \(\psi\) la función de salida (p.\,ej. softmax o identidad). La ecuación (\ref{eq:rnn_state}) explica cómo la red integra información nueva con su memoria previa.

En cada instante temporal \(t\) la RNN combina la entrada actual \(x_t\) con su memoria previa \(h_{t-1}\) para producir un nuevo estado \(h_t\). El estado en \(h_t\) resume la información relevante vista hasta el tiempo \(t\). El reuso de los mismos parámetros a lo largo del tiempo (weight sharing) permite que la red generalice a secuencias de distinta longitud y aprenda patrones temporales sin aprender parámetros independientes por posición.

Si definimos una pérdida por paso \( \ell(y_t, \hat{y}_t)\), la función de coste sobre una secuencia de longitud \(T\) es:
\begin{equation}
    \mathcal{L} = \sum_{t=1}^{T} \ell(y_t, \hat{y}_t).
    \label{eq:seq_loss}
\end{equation}
El entrenamiento se realiza mediante \emph{Backpropagation Through Time} (BPTT), que aplica la regla de la cadena a la red desplegada en el tiempo, acumulando gradientes en cada paso temporal.

Al calcular gradientes a través de muchas composiciones en el tiempo, las derivadas involucran productos iterados de jacobianos. Esto puede producir:
\begin{itemize}
    \item \textbf{Explosión del gradiente}: normas de gradiente que crecen exponencialmente, provocando inestabilidad en el entrenamiento.
    \item \textbf{Desvanecimiento del gradiente}: normas que decrecen exponencialmente, imposibilitando el aprendizaje de dependencias a largo plazo.
\end{itemize}
Formalmente, la contribución del paso \(k\) al gradiente en el tiempo \(t\) contiene términos del tipo
\[
\prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^{t} J_i,
\]
donde \(J_i\) es el jacobiano local \(\partial h_i/\partial h_{i-1}\). Si los autovalores de \(J_i\) son menores que 1 en módulo, el producto tiende a cero (desvanecimiento); si son mayores, tiende a infinito (explosión) \cite{pascanu2013difficulty}.
Para aprender dependencias a largo plazo sin sufrir tanto el problema del gradiente, se introdujeron celdas con \emph{puertas}.

\subsubsection{LSTM (Long Short-Term Memory)}
Una celda LSTM incorpora una \emph{celda de memoria} \(c_t\) y tres puertas (forget, input, output) que controlan cuánto de la memoria previa se olvida, cuánto de la nueva entrada se almacena y qué parte de la memoria se expone como salida. Una formulación compacta es:
\begin{align}
    f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
    i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
    o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
    \tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
    c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
    h_t &= o_t \odot \tanh(c_t),
\end{align}
donde \(\sigma\) es la sigmoide y \(\odot\) el producto elemento a elemento. Estas compuertas permiten mantener información relevante en \(c_t\) durante muchos pasos, facilitando dependencias de largo alcance \cite{hochreiter1997long}.

\subsubsection{GRU (Gated Recurrent Unit)}
La GRU simplifica la LSTM combinando algunas puertas y estados en una estructura más compacta. Sus ecuaciones típicas son:
\begin{align}
    z_t &= \sigma(W_z x_t + U_z h_{t-1} + b_z) \\
    r_t &= \sigma(W_r x_t + U_r h_{t-1} + b_r) \\
    \tilde{h}_t &= \tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h) \\
    h_t &= (1 - z_t)\odot h_{t-1} + z_t \odot \tilde{h}_t,
\end{align}
donde \(z_t\) es la \emph{update gate} y \(r_t\) la \emph{reset gate}. Las GRU a menudo logran desempeño comparable al de LSTM con menor número de parámetros \cite{cho2014learning}.

